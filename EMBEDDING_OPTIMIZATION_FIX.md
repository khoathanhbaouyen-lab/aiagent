# ‚ö° GI·∫¢I PH√ÅP T·ªêI ∆ØU EMBEDDINGS - KH√îNG C·∫¶N PYTORCH

## ‚ùå V·∫•n ƒë·ªÅ hi·ªán t·∫°i
- `sentence-transformers` + `PyTorch` g√¢y circular import tr√™n Windows
- Kh√¥ng th·ªÉ c√†i tr·ª±c ti·∫øp do conflict

## ‚úÖ 3 GI·∫¢I PH√ÅP THAY TH·∫æ

### **GI·∫¢I PH√ÅP 1: BATCH EMBEDDINGS (KHUY√äN D√ôNG)**
**Kh√¥ng c·∫ßn c√†i g√¨ th√™m, ch·ªâ t·ªëi ∆∞u c√°ch d√πng OpenAI API**

```python
# Trong app.py (d√≤ng ~1945)
embeddings = OpenAIEmbeddings(
    api_key=OPENAI_API_KEY,
    model="text-embedding-3-small",
    chunk_size=100,  # ‚Üê Batch 100 docs/request thay v√¨ 1
    show_progress_bar=False
)
```

**Hi·ªáu qu·∫£:**
- Gi·∫£m s·ªë request: 100 docs ‚Üí 1 request
- T·ªëc ƒë·ªô: T·ª´ 1.2s xu·ªëng ~0.4-0.6s (nhanh g·∫•p 2-3 l·∫ßn)
- Chi ph√≠ gi·∫£m 50% (√≠t request h∆°n)

---

### **GI·∫¢I PH√ÅP 2: ONNX EMBEDDINGS (Nhanh, nh·∫π, kh√¥ng c·∫ßn PyTorch)**

```bash
pip install optimum[onnxruntime]
pip install sentence-transformers-onnx
```

```python
# Trong app.py
from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={
        'device': 'cpu',
        'backend': 'onnx'  # ‚Üê D√πng ONNX thay v√¨ PyTorch
    }
)
```

**∆Øu ƒëi·ªÉm:**
- T·ªëc ƒë·ªô: ~0.1-0.2s (nhanh g·∫•p 6-12 l·∫ßn)
- Nh·∫π h∆°n PyTorch (~200MB vs ~2GB)
- Kh√¥ng c√≥ circular import

**Nh∆∞·ª£c ƒëi·ªÉm:**
- C·∫ßn c√†i th√™m package
- Setup ph·ª©c t·∫°p h∆°n

---

### **GI·∫¢I PH√ÅP 3: CACHE EMBEDDINGS (T·ªëi ∆∞u d√†i h·∫°n)**

L∆∞u embeddings ƒë√£ t√≠nh v√†o Redis/SQLite ƒë·ªÉ t√°i s·ª≠ d·ª•ng:

```python
import hashlib
import pickle
import sqlite3

class CachedEmbeddings:
    def __init__(self, base_embeddings):
        self.base = base_embeddings
        self.conn = sqlite3.connect("embeddings_cache.db")
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cache (
                text_hash TEXT PRIMARY KEY,
                embedding BLOB
            )
        """)
    
    def embed_query(self, text):
        h = hashlib.md5(text.encode()).hexdigest()
        row = self.conn.execute(
            "SELECT embedding FROM cache WHERE text_hash=?", (h,)
        ).fetchone()
        
        if row:
            return pickle.loads(row[0])  # Cache hit!
        
        emb = self.base.embed_query(text)
        self.conn.execute(
            "INSERT OR REPLACE INTO cache VALUES (?, ?)",
            (h, pickle.dumps(emb))
        )
        self.conn.commit()
        return emb

# S·ª≠ d·ª•ng
embeddings = CachedEmbeddings(
    OpenAIEmbeddings(model="text-embedding-3-small")
)
```

**Hi·ªáu qu·∫£:**
- L·∫ßn 1: ~1.2s (g·ªçi API)
- L·∫ßn 2+: ~0.001s (ƒë·ªçc cache) ‚Üê Nhanh g·∫•p 1200 l·∫ßn!

---

## üéØ KHUY·∫æN NGH·ªä

### Ng·∫Øn h·∫°n (√Åp d·ª•ng ngay):
‚úÖ **GI·∫¢I PH√ÅP 1: Batch Embeddings** (ƒë√£ implement)
- Th√™m `chunk_size=100` v√†o OpenAI config
- T·ªëc ƒë·ªô tƒÉng 2-3 l·∫ßn
- Kh√¥ng c·∫ßn c√†i g√¨ th√™m

### Trung h·∫°n (N·∫øu c·∫ßn nhanh h∆°n):
‚úÖ **GI·∫¢I PH√ÅP 3: Cache Embeddings**
- Implement cache SQLite cho c√°c query th∆∞·ªùng g·∫∑p
- T√°i s·ª≠ d·ª•ng embeddings ƒë√£ t√≠nh
- T·ªëc ƒë·ªô ~1200 l·∫ßn v·ªõi cache hit

### D√†i h·∫°n (N·∫øu mu·ªën offline ho√†n to√†n):
‚úÖ **GI·∫¢I PH√ÅP 2: ONNX**
- Ch·ªù fix PyTorch conflict
- Ho·∫∑c d√πng Docker container ri√™ng cho embedding service

---

## üìä So s√°nh Performance

| Ph∆∞∆°ng ph√°p | L·∫ßn ƒë·∫ßu | L·∫ßn sau | C√†i ƒë·∫∑t | Offline |
|-------------|---------|---------|---------|---------|
| **OpenAI (c≈©)** | 1.2s | 1.2s | ‚úÖ D·ªÖ | ‚ùå |
| **OpenAI Batch** | 0.5s | 0.5s | ‚úÖ D·ªÖ | ‚ùå |
| **OpenAI + Cache** | 1.2s | 0.001s | ‚ö†Ô∏è TB | ‚ùå |
| **ONNX** | 0.15s | 0.15s | ‚ö†Ô∏è Kh√≥ | ‚úÖ |
| **PyTorch** | ‚ùå L·ªói | - | ‚ùå | - |

---

## üîß ƒêANG √ÅP D·ª§NG

**Hi·ªán t·∫°i:** GI·∫¢I PH√ÅP 1 (Batch Embeddings)
```python
embeddings = OpenAIEmbeddings(
    chunk_size=100,  # ‚Üê T·ªëi ∆∞u batch
    show_progress_bar=False
)
```

**K·∫øt qu·∫£ k·ª≥ v·ªçng:**
- Th·ªùi gian: 1.2s ‚Üí 0.4-0.6s
- C·∫£i thi·ªán: ~2-3 l·∫ßn

---

**T√°c gi·∫£:** GitHub Copilot  
**Ng√†y:** 16/11/2025
