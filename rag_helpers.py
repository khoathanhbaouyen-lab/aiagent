# rag_helpers.py
import os
import re
import json
import pytz
import unidecode
from datetime import datetime
from typing import List, Tuple, Optional, Union, Dict, Any
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langchain_core.language_models.chat_models import BaseChatModel
# Import embeddings Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o tá»« helpers.py
from helpers import embeddings, OPENAI_API_KEY

# --- Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN (Láº¥y tá»« codeold.py) ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# 1. ThÆ° má»¥c Má»šI chá»©a Táº¤T Cáº¢ dá»¯ liá»‡u riÃªng cá»§a ngÆ°á»i dÃ¹ng
USER_DATA_ROOT = os.path.join(BASE_DIR, "user_data")
os.makedirs(USER_DATA_ROOT, exist_ok=True)

# 2. CÃ¡c thÆ° má»¥c con (Vector)
USER_VECTOR_DB_ROOT = os.path.join(USER_DATA_ROOT, "vector_db")
os.makedirs(USER_VECTOR_DB_ROOT, exist_ok=True)

# 3. ThÆ° má»¥c Tá»« Ä‘iá»ƒn Fact
USER_FACT_DICTS_ROOT = os.path.join(USER_DATA_ROOT, "fact_dictionaries")
os.makedirs(USER_FACT_DICTS_ROOT, exist_ok=True)

# Timezone VN
VN_TZ = pytz.timezone("Asia/Ho_Chi_Minh")


# --- HÃ€M HELPER CHUNG (Láº¥y tá»« codeold.py) ---

def _timestamp() -> str:
    """Láº¥y timestamp (dÃ¹ng cho V94/V97)"""
    return datetime.now().strftime('%Y%m%d-%H%M%S')

def _sanitize_user_id_for_path(user_email: str) -> str:
    """Biáº¿n email thÃ nh tÃªn thÆ° má»¥c an toÃ n."""
    # Thay @ vÃ  . báº±ng _
    safe_name = re.sub(r"[@\.]", "_", user_email)
    # XÃ³a cÃ¡c kÃ½ tá»± khÃ´ng an toÃ n cÃ²n láº¡i
    return re.sub(r"[^a-zA-Z0-9_\-]", "", safe_name)


# --- 1. HÃ€M CHÃNH Báº N YÃŠU Cáº¦U ---
# (Láº¥y tá»« codeold.py)

def get_user_vector_dir(user_id_str: str) -> str:
    """Láº¥y Ä‘Æ°á»ng dáº«n thÆ° má»¥c vector DB cá»§a user (vÃ  táº¡o náº¿u chÆ°a cÃ³)."""
    safe_user_dir = _sanitize_user_id_for_path(user_id_str)
    user_dir = os.path.join(USER_VECTOR_DB_ROOT, safe_user_dir)
    os.makedirs(user_dir, exist_ok=True)
    return user_dir

def get_user_vectorstore_retriever(user_id_str: str) -> Tuple[Chroma, Any]:
    """
    Khá»Ÿi táº¡o Vectorstore vÃ  Retriever cho 1 user cá»¥ thá»ƒ.
    (Láº¥y tá»« codeold.py)
    """
    global embeddings
    if embeddings is None:
        raise ValueError("Lá»—i: Embeddings chÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o (OPENAI_API_KEY cÃ³ thá»ƒ bá»‹ thiáº¿u).")

    persist_directory = get_user_vector_dir(user_id_str)
    
    vectorstore = Chroma(
        persist_directory=persist_directory,
        embedding_function=embeddings,
        collection_name="memory"
    )
    # Láº¥y K=20 (theo logic V96)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 20})
    
    print(f"âœ… VectorStore cho user '{user_id_str}' Ä‘Ã£ sáºµn sÃ ng táº¡i {persist_directory} (mode=Similarity K=20)")
    return vectorstore, retriever


# --- 2. HÃ€M KHá»I Táº O LLM ---
# (Láº¥y tá»« codeold.py)

def get_user_llm() -> ChatOpenAI:
    """
    Khá»Ÿi táº¡o LLM (logic) cho user.
    """
    return ChatOpenAI(
        model="gpt-4.1-mini", 
        temperature=0, 
        api_key=OPENAI_API_KEY
    )


# --- 3. CÃC HÃ€M RAG HELPER KHÃC (Äá»‚ `index.py` VÃ€ `chat_logic.py` HOáº T Äá»˜NG) ---

# --- Fact Map (Tá»« Ä‘iá»ƒn Fact) Helpers ---
# (Láº¥y tá»« codeold.py)

def get_user_fact_dict_path(user_id_str: str) -> str:
    """Láº¥y Ä‘Æ°á»ng dáº«n file JSON tá»« Ä‘iá»ƒn fact cá»§a user."""
    safe_name = _sanitize_user_id_for_path(user_id_str)
    # Sá»­a: DÃ¹ng USER_FACT_DICTS_ROOT thay vÃ¬ get_user_vector_dir
    user_dir = os.path.join(USER_FACT_DICTS_ROOT, safe_name)
    os.makedirs(user_dir, exist_ok=True)
    return os.path.join(user_dir, "fact_map.json")

def load_user_fact_dict(user_id_str: str) -> dict:
    """Táº£i tá»« Ä‘iá»ƒn fact cá»§a user tá»« file JSON. (Sá»­a lá»—i V94)"""
    path = get_user_fact_dict_path(user_id_str)
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ Lá»—i Ä‘á»c fact dict {user_id_str}: {e}")
            try:
                bad_file_path = f"{path}.{_timestamp()}.corrupted"
                os.rename(path, bad_file_path)
                print(f"âœ… ÄÃ£ di dá»i file há»ng sang: {bad_file_path}")
            except Exception as e_rename:
                print(f"âŒ KhÃ´ng thá»ƒ di dá»i file há»ng: {e_rename}")
    return {}

def save_user_fact_dict(user_id_str: str, data: dict):
    """LÆ°u tá»« Ä‘iá»ƒn fact cá»§a user vÃ o file JSON."""
    path = get_user_fact_dict_path(user_id_str)
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ Lá»—i lÆ°u fact dict {user_id_str}: {e}")

async def call_llm_to_classify(
    llm: ChatOpenAI, 
    question: str, 
    fact_map: dict
) -> Tuple[str, str, str]:
    """
    (Sá»¬A Lá»–I V88 - Láº¥y tá»« codeold.py)
    """
    existing_facts_str = "Context (Fact) hiá»‡n táº¡i:\n(KhÃ´ng cÃ³)"
    try:
        if fact_map and isinstance(fact_map, dict):
            existing_facts_list = []
            seen_keys = set()
            for data in fact_map.values():
                if isinstance(data, dict):
                    key = data.get("key")
                    label = data.get("label")
                    if key and key not in seen_keys:
                        existing_facts_list.append(f"- Key: {key} (Label: {label})")
                        seen_keys.add(key)
                elif isinstance(data, str) and data not in seen_keys:
                    label = data.replace("_", " ").title()
                    existing_facts_list.append(f"- Key: {data} (Label: {label})")
                    seen_keys.add(data)
            if existing_facts_list:
                existing_facts_str = "Context (Fact) hiá»‡n táº¡i:\n" + "\n".join(sorted(existing_facts_list))
    except Exception as e_parse:
        print(f"âš ï¸ Lá»—i parse fact_map (V88): {e_parse}")
        existing_facts_str = "Context (Fact) hiá»‡n táº¡i:\n(Lá»—i parse)"
        
    prompt_text = f"""
    Báº¡n lÃ  má»™t chuyÃªn gia PhÃ¢n tÃ­ch Query (Classifier).
    Query: "{question}"
    {existing_facts_str}
    NHIá»†M Vá»¤:
    1. Äá»c ká»¹ Query vÃ  Context (Fact) hiá»‡n táº¡i.
    2. Æ¯U TIÃŠN 1 (TÃ¡i sá»­ dá»¥ng): Náº¿u Query cÃ³ váº» thuá»™c vá» má»™t "Fact" Ä‘Ã£ cÃ³ trong Context, hÃ£y TÃI Sá»¬ Dá»¤NG 'Key' vÃ  'Label' cá»§a nÃ³.
    3. Æ¯U TIÃŠN 2 (Táº¡o má»›i): Náº¿u Query khÃ´ng khá»›p vá»›i Context, hÃ£y Táº O Má»šI má»™t 'Key' vÃ  'Label' há»£p lÃ½.
    4. TrÃ­ch xuáº¥t 'core_query_term' (tá»« khÃ³a tÃ¬m kiáº¿m chÃ­nh, Ä‘Ã£ loáº¡i bá» hÃ nh Ä‘á»™ng vÃ  danh má»¥c).
    QUY Táº®C TRáº¢ Vá»€:
    - Äá»‹nh dáº¡ng: `fact_key | Label Tiáº¿ng Viá»‡t | core_query_term`
    - KHÃ”NG GIáº¢I THÃCH.
    VÃ Dá»¤ TÃI Sá»¬ Dá»¤NG (Ráº¤T QUAN TRá»ŒNG):
    Query: "xem áº£nh phan thiet"
    Context (Fact) hiá»‡n táº¡i:
    - Key: du_lich (Label: Du Lá»‹ch)
    (GPT sáº½ tháº¥y 'phan thiet' liÃªn quan Ä‘áº¿n 'du_lich')
    Output: du_lich | Du Lá»‹ch | anh phan thiet
    VÃ Dá»¤ Táº O Má»šI:
    Query: "pass server cá»§a tÃ´i"
    Context (Fact) hiá»‡n táº¡i:
    - Key: du_lich (Label: Du Lá»‹ch)
    Output: server_thong_tin | Server ThÃ´ng Tin | pass server
    VÃ Dá»¤ Lá»ŒC (CHUNG):
    Query: "xem file trong cong viec"
    Context (Fact) hiá»‡n táº¡i:
    - Key: cong_viec (Label: CÃ´ng Viá»‡c)
    Output: cong_viec | CÃ´ng Viá»‡c | ALL
    Output (key | label | core_query_term):
    """
    
    try:
        # Sá»­a: DÃ¹ng ainvoke (async)
        resp = await llm.ainvoke(prompt_text)
        raw_output = resp.content.strip().strip("`'\"")
        
        fact_key = "general"
        fact_label = "General"
        core_query_term = question
        
        if "|" in raw_output:
            parts = raw_output.split("|")
            if len(parts) >= 3:
                key_part = parts[0].strip().replace(" ", "_")
                label_part = parts[1].strip()
                name_part = parts[2].strip()
                if key_part: fact_key = re.sub(r"[^a-z0-9_]", "", key_part.lower())
                if label_part: fact_label = label_part
                if name_part: core_query_term = name_part
            elif len(parts) == 2:
                key_part = parts[0].strip().replace(" ", "_")
                label_part = parts[1].strip()
                if key_part: fact_key = re.sub(r"[^a-z0-9_]", "", key_part.lower())
                if label_part: fact_label = label_part
                core_query_term = "ALL"
        else:
            key_part = raw_output.replace(" ", "_")
            fact_key = re.sub(r"[^a-z0-9_]", "", key_part.lower())
            fact_label = fact_key
            core_query_term = "ALL"
            
        if not fact_key: fact_key = "general"
        if not fact_label: fact_label = "General"
        if not core_query_term: core_query_term = question
        
        print(f"[call_llm_to_classify] (Prompt V88) Query: '{question}' -> Key: '{fact_key}' | Label: '{fact_label}' | CoreQuery: '{core_query_term}'")
        return fact_key, fact_label, core_query_term
        
    except Exception as e:
        print(f"âŒ Lá»—i call_llm_to_classify (V88): {e}")
        return "general", "General", question

# --- RAG Filter Helpers ---
# (Láº¥y tá»« codeold.py)

def _build_rag_filter_from_query(query: str) -> Optional[dict]:
    """(Sá»¬A Lá»–I V89)
    DÃ¹ng regex Ä‘á»ƒ tÃ¬m Tá»ª KHÃ“A (word) 'anh'/'hinh'/'file'.
    Lá»c theo 'entry_type': 'file_master'.
   
    """
    q_low = unidecode.unidecode(query.lower())
    
    # 1. Æ¯u tiÃªn: TÃ¬m (chá»‰) áº£nh
    if re.search(r"\b(anh|hinh|images?|imgs?)\b", q_low):
         print(f"[_build_rag_filter] (Sá»­a lá»—i V89) PhÃ¡t hiá»‡n lá»c (chá»‰) áº£nh Gá»C (Regex).")
         return {
             "$and": [
                 {"file_type": "image"},
                 {"entry_type": "file_master"}
             ]
         }

    # 2. TÃ¬m file Gá»C
    file_keywords = [
        "file", "excel", "xlsx", "xls", "trang tinh", 
        "word", "docx", "doc", "van ban", 
        "pdf", "tai lieu", "danh sach", "ds"
    ]
    if any(re.search(r"\b" + re.escape(kw) + r"\b", q_low) for kw in file_keywords):
         print(f"[_build_rag_filter] (Sá»­a lá»—i V89) PhÃ¡t hiá»‡n lá»c file Gá»C (master) (Regex).")
         return {"entry_type": "file_master"}
         
    # 3. KhÃ´ng phÃ¡t hiá»‡n
    return None

def _helper_sort_results_by_timestamp(
    ids: List[str], 
    docs: List[str], 
    metadatas: List[dict]
) -> List[tuple[str, str, dict]]:
    """
    (Má»šI - V94) Helper: Sáº¯p xáº¿p káº¿t quáº£ Chroma
    theo 'timestamp' (má»›i nháº¥t lÃªn Ä‘áº§u).
   
    """
    temp_results_list = []
    
    # 1. Gá»™p 3 list láº¡i
    for doc_id, content, metadata in zip(ids, docs, metadatas):
        ts_str = "1970-01-01T00:00:00+00:00" # Má»‘c Unix (cho data cÅ©)
        if metadata and metadata.get("timestamp"):
            ts_str = metadata.get("timestamp")
        
        temp_results_list.append({
            "id": doc_id, 
            "content": content, 
            "metadata": metadata, 
            "timestamp_str": ts_str
        })
    
    # 2. Sáº¯p xáº¿p (má»›i nháº¥t -> cÅ© nháº¥t)
    try:
        sorted_temp_list = sorted(
            temp_results_list, 
            key=lambda x: x["timestamp_str"], 
            reverse=True
        )
    except Exception as e_sort:
        print(f"âš ï¸ Lá»—i khi sáº¯p xáº¿p timestamp (V94 Helper): {e_sort}. DÃ¹ng danh sÃ¡ch gá»‘c.")
        sorted_temp_list = temp_results_list

    # 3. Tráº£ vá» dáº¡ng list of tuples
    return [
        (item["id"], item["content"], item["metadata"]) 
        for item in sorted_temp_list
    ]
# chat_logic.py (THAY THáº¾ HÃ€M NÃ€Y - KHOáº¢NG DÃ’NG 239)

def _llm_filter_for_selection(
    llm: ChatOpenAI,
    query: str,
    candidates: List[dict] # List of {"id": str, "name": str, "note": str, "metadata": dict}
) -> List[dict]:
    """
    (Sá»¬A Lá»–I V104 - FIX Lá»–I "RA Háº¾T")
    DÃ¹ng LLM (sync) Ä‘á»ƒ lá»c Káº¾T QUáº¢ TÃŒM KIáº¾M (cho file/áº£nh).
    Quy táº¯c V104: Æ¯u tiÃªn TÃŠN, náº¿u TÃªn khÃ´ng khá»›p thÃ¬ kiá»ƒm tra GHI CHÃš.
    """
    if not candidates:
        return []
        
    # 1. Táº¡o danh sÃ¡ch á»©ng viÃªn
    candidate_list_str = "\n".join([
        f"<item id='{item['id']}'>TÃªn: {item['name']} | Ghi chÃº: {item['note']}</item>"
        for item in candidates
    ])
    
    # --- ğŸš€ Báº®T Äáº¦U Sá»¬A Lá»–I V104 (PROMPT Má»šI) ğŸš€ ---
    prompt = f"""Báº¡n lÃ  má»™t bá»™ lá»c thÃ´ng minh (Smart Filter).
Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  Lá»ŒC danh sÃ¡ch (Context) dá»±a trÃªn YÃªu cáº§u (Query).

YÃªu cáº§u (Query): "{query}"

Danh sÃ¡ch á»©ng viÃªn (Context):
{candidate_list_str}

QUY Táº®C Lá»ŒC (V104):
1. Äá»c ká»¹ YÃªu cáº§u (Query).
2. So sÃ¡nh Query vá»›i Tá»ªNG item.
3. Æ¯U TIÃŠN GIá»® Láº I (Keep) náº¿u PHáº¦N TÃŠN (Name) khá»›p vá»›i Query.
   (VÃ­ dá»¥: Query 'mÃ¡y khoan' khá»›p TÃªn 'áº£nh mÃ¡y khoan bosch').
4. Náº¾U TÃŠN KHÃ”NG KHá»šP, hÃ£y kiá»ƒm tra PHáº¦N GHI CHÃš (Note). Giá»¯ láº¡i (Keep) náº¿u Ghi chÃº khá»›p vá»›i Query.
   (VÃ­ dá»¥: Query 'mÃ¡y khoan' khá»›p Ghi chÃº 'Ä‘Ã¢y lÃ  mÃ¡y khoan').
5. Bá» QUA (Discard) náº¿u cáº£ TÃªn vÃ  Ghi chÃº Ä‘á»u khÃ´ng liÃªn quan.

VÃ Dá»¤ QUAN TRá»ŒNG (V104):
Query: "áº£nh mÃ¡y khoan"
Context:
<item id='abc'>TÃªn: anh may khoan bosch | Ghi chÃº: ...</item>
<item id='def'>TÃªn: IMG_1234.jpg | Ghi chÃº: luu anh may khoan</item>
<item id='xyz'>TÃªn: anh du lich vung tau | Ghi chÃº: ...</item>

Output (Chá»‰ tráº£ vá» ID):
abc
def

Output (Chá»‰ tráº£ vá» cÃ¡c ID, má»—i ID má»™t dÃ²ng. KHÃ”NG GIáº¢I THÃCH):
"""
    # --- ğŸš€ Káº¾T THÃšC Sá»¬A Lá»–I V104 ğŸš€ ---
    
    try:
        # 3. Gá»i LLM (sync)
        resp = llm.invoke(prompt)
        llm_output_text = resp.content.strip()
        
        if not llm_output_text:
            return []
            
        # 4. Lá»c láº¡i
        llm_approved_ids = set([line.strip() for line in llm_output_text.split('\n') if line.strip()])
        
        final_list = []
        for candidate in candidates:
            if candidate['id'] in llm_approved_ids:
                final_list.append(candidate)
                
        print(f"[LLM Filter Selection] (V104) ÄÃ£ lá»c {len(candidates)} -> cÃ²n {len(final_list)} (Query: '{query}')")
        return final_list
        
    except Exception as e:
        print(f"âŒ Lá»—i _llm_filter_for_selection (V104): {e}")
        # An toÃ n: tráº£ vá» rá»—ng náº¿u LLM lá»—i
        return []

# rag_helpers.py

# ... (Giá»¯ nguyÃªn cÃ¡c hÃ m Ä‘Ã£ cÃ³) ...

# ğŸš€ Báº®T Äáº¦U: THÃŠM 2 HÃ€M Má»šI Tá»ª CODEOLD.PY VÃ€O CUá»I FILE ğŸš€

async def _llm_batch_split_classify(
    llm: BaseChatModel, 
    user_note: str, 
    num_files: int
) -> List[dict]:
    """
    (Má»šI - Sá»¬A Lá»–I 79 Tá»ª CODEOLD)
    Má»™t lá»‡nh gá»i GPT duy nháº¥t Ä‘á»ƒ TÃCH vÃ  PHÃ‚N LOáº I
    cho 'Smart Mode' (khi khÃ´ng cÃ³ 'vÃ o má»¥c').
    Tráº£ vá» list of dicts: 
    [{"name": "...", "key": "...", "label": "..."}, ...]
    """
    
    prompt = f"""
    Ghi chÃº cá»§a ngÆ°á»i dÃ¹ng: "{user_note}"
    Sá»‘ lÆ°á»£ng file Ä‘Ã£ upload: {num_files}

    Nhiá»‡m vá»¥: 
    1. PhÃ¢n tÃ­ch Ghi chÃº Ä‘á»ƒ tÃ¬m ra ngá»¯ cáº£nh chung (vÃ­ dá»¥: 'cÃ´ng viá»‡c').
    2. TÃ¡ch Ghi chÃº thÃ nh chÃ­nh xÃ¡c {num_files} TÃŠN (name) riÃªng láº».
    3. Tráº£ vá» Má»–I file trÃªn Má»˜T DÃ’NG theo Ä‘á»‹nh dáº¡ng:
       `Ten file da tach | fact_key (snake_case) | Fact Label (Tieng Viet)`

    QUY Táº®C:
    - Pháº£i tráº£ vá» ÄÃšNG {num_files} dÃ²ng.
    - PHáº¢I Ã¡p dá»¥ng ngá»¯ cáº£nh chung (vÃ­ dá»¥: 'cong viec') cho Táº¤T Cáº¢ cÃ¡c dÃ²ng.
    - KHÃ”NG giáº£i thÃ­ch.

    VÃ­ dá»¥ 1:
    Ghi chÃº: "luu file ns 2024 vÃ  ns 2025 vao cong viec"
    Sá»‘ lÆ°á»£ng file: 2
    Output:
    file ns 2024 | cong_viec | CÃ´ng Viá»‡c
    file ns 2025 | cong_viec | CÃ´ng Viá»‡c

    VÃ­ dá»¥ 2:
    Ghi chÃº: "anh cccd mat truoc va mat sau vao ho so ca nhan"
    Sá»‘ lÆ°á»£ng file: 2
    Output:
    anh cccd mat truoc | ho_so_ca_nhan | Há»“ SÆ¡ CÃ¡ NhÃ¢n
    anh cccd mat sau | ho_so_ca_nhan | Há»“ SÆ¡ CÃ¡ NhÃ¢n
    
    VÃ­ dá»¥ 3 (Fallback - KhÃ´ng cÃ³ ngá»¯ cáº£nh):
    Ghi chÃº: "hai file linh tinh"
    Sá»‘ lÆ°á»£ng file: 2
    Output:
    hai file linh tinh 1 | general | General
    hai file linh tinh 2 | general | General
    """
    
    results = []
    try:
        resp = await llm.ainvoke(prompt)
        lines = [line.strip() for line in resp.content.strip().split('\n') if line.strip()]
        
        if len(lines) == num_files:
            print(f"âœ… [LLM Batch Split] (Sá»­a lá»—i 79) GPT Ä‘Ã£ tÃ¡ch vÃ  phÃ¢n loáº¡i {len(lines)} má»¥c.")
            for line in lines:
                parts = line.split("|")
                if len(parts) >= 3:
                    results.append({
                        "name": parts[0].strip(),
                        "key": parts[1].strip(),
                        "label": parts[2].strip()
                    })
                else:
                    results.append({"name": line, "key": "general", "label": "General"})
            return results
        
        print(f"âš ï¸ [LLM Batch Split] (Sá»­a lá»—i 79) GPT tráº£ vá» {len(lines)} dÃ²ng (mong Ä‘á»£i {num_files}). DÃ¹ng fallback.")

    except Exception as e:
        print(f"âŒ Lá»—i _llm_batch_split_classify: {e}. DÃ¹ng fallback.")

    return [] # Tráº£ vá» list rá»—ng Ä‘á»ƒ kÃ­ch hoáº¡t fallback


async def _llm_split_notes(
    llm: BaseChatModel, 
    user_note: str, 
    num_files: int
) -> List[str]:
    """
    (Má»šI - Tá»ª CODEOLD)
    DÃ¹ng LLM Ä‘á»ƒ tÃ¡ch ghi chÃº chung thÃ nh cÃ¡c ghi chÃº con
    tÆ°Æ¡ng á»©ng vá»›i sá»‘ lÆ°á»£ng file (DÃ¹ng cho Album Mode).
    """
    if num_files == 1:
        return [user_note]
        
    prompt = f"""
    Ghi chÃº cá»§a ngÆ°á»i dÃ¹ng: "{user_note}"
    Sá»‘ lÆ°á»£ng file Ä‘Ã£ upload: {num_files}

    Nhiá»‡m vá»¥: TÃ¡ch "Ghi chÃº cá»§a ngÆ°á»i dÃ¹ng" thÃ nh chÃ­nh xÃ¡c {num_files} pháº§n ghi chÃº riÃªng láº», 
    tÆ°Æ¡ng á»©ng vá»›i {num_files} file theo Ä‘Ãºng thá»© tá»±.

    QUAN TRá»ŒNG:
    - Tráº£ vá» Má»–I ghi chÃº trÃªn Má»˜T DÃ’NG.
    - KHÃ”NG giáº£i thÃ­ch.
    - Náº¿u khÃ´ng thá»ƒ tÃ¡ch (vÃ­ dá»¥: ghi chÃº chung chung), 
      hÃ£y láº·p láº¡i ghi chÃº gá»‘c {num_files} láº§n.

    VÃ­ dá»¥ 1:
    Ghi chÃº: "luu 2 anh du lich vÅ©ng tÃ u vÃ  ha long"
    Sá»‘ lÆ°á»£ng file: 2
    Output:
    anh du lich vung tau
    anh du lich ha long

    VÃ­ dá»¥ 2:
    Ghi chÃº: "file hop dong, file bao gia"
    Sá»‘ lÆ°á»£ng file: 2
    Output:
    file hop dong
    file bao gia
    
    VÃ­ dá»¥ 3 (Fallback):
    Ghi chÃº: "áº£nh du lá»‹ch cá»§a tÃ´i"
    Sá»‘ lÆ°á»£ng file: 2
    Output:
    áº£nh du lá»‹ch cá»§a tÃ´i
    áº£nh du lá»‹ch cá»§a tÃ´i
    """
    try:
        resp = await llm.ainvoke(prompt)
        lines = [line.strip() for line in resp.content.strip().split('\n') if line.strip()]
        
        if len(lines) == num_files:
            print(f"âœ… [LLM Split] ÄÃ£ tÃ¡ch '{user_note}' -> {lines}")
            return lines
            
        print(f"âš ï¸ [LLM Split] TÃ¡ch tháº¥t báº¡i (tráº£ vá» {len(lines)}), dÃ¹ng fallback.")
        return [user_note] * num_files 
        
    except Exception as e:
        print(f"âŒ Lá»—i _llm_split_notes: {e}. DÃ¹ng fallback.")
        return [user_note] * num_files
# ğŸš€ Káº¾T THÃšC: THÃŠM 2 HÃ€M Má»šI