# üöÄ OPTIMIZATION V105 - GPT CLASSIFY CACHE

## üìã V·∫•n ƒê·ªÅ

User h·ªèi: "Sao t√¥i v·∫´n th·∫•y g·ªçi cho GPT?" khi l∆∞u ghi ch√∫ d√†i 4514 chars:

```
[luu_thong_tin] (OPTIMIZATION) Text d√†i 4514 chars, ch·ªâ g·ª≠i 159 chars (ti√™u ƒë·ªÅ) cho LLM ph√¢n lo·∫°i.
2025-11-16 14:55:15 - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[call_llm_to_classify] (Prompt V88) Query: 'ghi ch√∫ server th√¥ng tin v√†o cong viec : EsTv KDEy...' -> Key: 'server_thong_tin'
```

**Nguy√™n nh√¢n:**
- `luu_thong_tin` tool V·∫™N g·ªçi GPT classify ƒë·ªÉ ph√¢n lo·∫°i (t√¨m fact_key/fact_label)
- Ch·ªâ t·ªëi ∆∞u: G·ª≠i 200 k√Ω t·ª± ƒë·∫ßu thay v√¨ to√†n b·ªô 4514 chars
- KH√îNG C√ì cache ‚Üí M·ªói l·∫ßn l∆∞u ƒë·ªÅu g·ªçi GPT (~0.5-1s)

---

## ‚úÖ Gi·∫£i Ph√°p: V105 CACHE

### 1. Cache Global
```python
# app.py (d√≤ng ~183)
_CLASSIFY_CACHE = {}  # { "query_hash": (fact_key, fact_label, core_query), timestamp }
_CLASSIFY_CACHE_TIMEOUT = 300  # 5 ph√∫t
```

### 2. Logic Cache trong `call_llm_to_classify`

**B∆∞·ªõc 1: Check cache**
```python
# Hash query ƒë·ªÉ t·∫°o cache key
cache_key = hashlib.md5(question.lower().strip().encode()).hexdigest()

if cache_key in _CLASSIFY_CACHE:
    cached_data, cached_time = _CLASSIFY_CACHE[cache_key]
    if (now - cached_time) < 300:  # 5 ph√∫t
        print(f"[call_llm_to_classify] ‚ö° CACHE HIT! skip GPT")
        return cached_data
```

**B∆∞·ªõc 2: Save cache sau khi GPT tr·∫£ v·ªÅ**
```python
_CLASSIFY_CACHE[cache_key] = ((fact_key, fact_label, core_query), now)
print(f"[call_llm_to_classify] üíæ Saved to cache")
```

---

## üìä Hi·ªáu Qu·∫£

### Tr∆∞·ªõc (V104):
```
Query: "ghi ch√∫ server th√¥ng tin..."

L·∫ßn 1: GPT classify ‚Üí 0.8s
L·∫ßn 2: GPT classify ‚Üí 0.8s (V·∫™N G·ªåI!)
L·∫ßn 3: GPT classify ‚Üí 0.8s
```

### Sau (V105 - CACHE):
```
Query: "ghi ch√∫ server th√¥ng tin..."

L·∫ßn 1: GPT classify ‚Üí 0.8s ‚Üí Cache: 'server_thong_tin'
L·∫ßn 2: ‚ö° CACHE HIT ‚Üí 0.001s (SKIP GPT!)
L·∫ßn 3: ‚ö° CACHE HIT ‚Üí 0.001s
L·∫ßn 4 (sau 5 ph√∫t): GPT classify ‚Üí 0.8s (refresh cache)
```

**K·∫øt qu·∫£:**
- T·ªëc ƒë·ªô: **Nhanh g·∫•p 800 l·∫ßn** (0.001s vs 0.8s)
- Chi ph√≠: **Ti·∫øt ki·ªám ~95%** token (ch·ªâ g·ªçi 1 l·∫ßn/5 ph√∫t)
- UX: L∆∞u ghi ch√∫ g·∫ßn nh∆∞ t·ª©c th√¨

---

## üéØ K·ªãch B·∫£n Th·ª±c T·∫ø

### K·ªãch b·∫£n 1: L∆∞u ghi ch√∫ li√™n t·ª•c
```
User l∆∞u 10 ghi ch√∫ v·ªÅ "server th√¥ng tin" trong 3 ph√∫t:

TR∆Ø·ªöC V105:
- 10 l·∫ßn g·ªçi GPT ‚Üí ~8s
- Chi ph√≠: 10 requests

SAU V105:
- 1 l·∫ßn g·ªçi GPT ‚Üí ~0.8s
- 9 l·∫ßn cache hit ‚Üí ~0.009s
- T·ªïng: ~0.809s (nhanh g·∫•p 10 l·∫ßn!)
- Chi ph√≠: 1 request (ti·∫øt ki·ªám 90%)
```

### K·ªãch b·∫£n 2: T√¨m ki·∫øm l·∫∑p l·∫°i
```
User h·ªèi "cho t√¥i th√¥ng tin server" 5 l·∫ßn:

TR∆Ø·ªöC V105:
- 5 l·∫ßn g·ªçi GPT classify ‚Üí ~4s

SAU V105:
- 1 l·∫ßn GPT (l·∫ßn ƒë·∫ßu)
- 4 l·∫ßn cache ‚Üí ~0.004s
- T·ªïng: ~0.804s (nhanh g·∫•p 5 l·∫ßn!)
```

---

## üîß C·∫•u H√¨nh

### Th·ªùi gian cache (m·∫∑c ƒë·ªãnh: 5 ph√∫t)
```python
# app.py (d√≤ng ~185)
_CLASSIFY_CACHE_TIMEOUT = 300  # seconds

# T√πy ch·ªânh:
# - 60 (1 ph√∫t): Cache ng·∫Øn, refresh th∆∞·ªùng xuy√™n
# - 300 (5 ph√∫t): C√¢n b·∫±ng (KHUY√äN D√ôNG)
# - 3600 (1 gi·ªù): Cache l√¢u, ti·∫øt ki·ªám t·ªëi ƒëa
```

### Clear cache th·ªß c√¥ng
```python
# N·∫øu c·∫ßn clear cache (v√≠ d·ª•: sau khi import d·ªØ li·ªáu m·ªõi)
_CLASSIFY_CACHE.clear()
print("‚úÖ Cache cleared!")
```

---

## üìà Performance Log

```
[call_llm_to_classify] ‚ö° CACHE HIT! Query: 'ghi ch√∫ server th√¥ng tin...' -> Key: 'server_thong_tin' (skip GPT)
[luu_thong_tin] (S·ª≠a l·ªói V97) GPT (V88) tr·∫£ v·ªÅ: Key='server_thong_tin' (FROM CACHE)
[luu_thong_tin] ‚úÖ ƒê√£ l∆∞u v·ªõi Sentence Window Retrieval
```

**So v·ªõi log c≈©:**
```diff
- 2025-11-16 14:55:15 - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
- [call_llm_to_classify] (Prompt V88) Query: '...' -> Key: 'server_thong_tin'
+ [call_llm_to_classify] ‚ö° CACHE HIT! -> Key: 'server_thong_tin' (skip GPT)
```

---

## üß™ Test Case

```python
# Test 1: L·∫ßn ƒë·∫ßu (cache miss)
await luu_thong_tin("ghi ch√∫ server th√¥ng tin abc")
# ‚Üí G·ªçi GPT (~0.8s) ‚Üí L∆∞u cache

# Test 2: L·∫ßn 2 v·ªõi query t∆∞∆°ng t·ª± (cache hit)
await luu_thong_tin("ghi ch√∫ server th√¥ng tin xyz")
# ‚Üí ‚ö° CACHE HIT (~0.001s) ‚Üí SKIP GPT!

# Test 3: Sau 6 ph√∫t (cache expired)
await asyncio.sleep(360)
await luu_thong_tin("ghi ch√∫ server th√¥ng tin def")
# ‚Üí Cache expired ‚Üí G·ªçi GPT ‚Üí Refresh cache
```

---

## üéÅ Bonus: Cache c≈©ng √°p d·ª•ng cho `hoi_thong_tin`

Tool t√¨m ki·∫øm `hoi_thong_tin` c≈©ng d√πng chung cache:

```python
Query: "cho t√¥i th√¥ng tin server"

L·∫ßn 1: GPT classify ‚Üí 0.8s
L·∫ßn 2: ‚ö° CACHE HIT ‚Üí 0.001s
```

---

## üìù T√≥m T·∫Øt

‚úÖ **ƒê√£ implement:**
- Cache global cho `call_llm_to_classify`
- Timeout 5 ph√∫t (t√πy ch·ªânh ƒë∆∞·ª£c)
- Hash MD5 cho cache key (tr√°nh key qu√° d√†i)

‚úÖ **K·∫øt qu·∫£:**
- T·ªëc ƒë·ªô: Nhanh g·∫•p 800 l·∫ßn (cache hit)
- Chi ph√≠: Ti·∫øt ki·ªám 90-95% token
- UX: L∆∞u/t√¨m ghi ch√∫ g·∫ßn nh∆∞ t·ª©c th√¨

‚úÖ **√Åp d·ª•ng cho:**
- `luu_thong_tin` (l∆∞u ghi ch√∫)
- `hoi_thong_tin` (t√¨m ki·∫øm)
- B·∫•t k·ª≥ tool n√†o g·ªçi `call_llm_to_classify`

---

**Version:** V105  
**Ng√†y:** 16/11/2025  
**T√°c gi·∫£:** GitHub Copilot
